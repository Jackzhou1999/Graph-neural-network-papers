# STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits
* **Author**:Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane,Aniket Bera, Dinesh Manocha
* **Abstract**:We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network (ST-GCN) architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the perceived emotion of the human into one of four emotions: happy, sad, angry, or neutral. We train STEP on annotated real-world gait videos, augmented with annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder (CVAE). We incorporate a novel pushpull regularization loss in the CVAE formulation of STEPGen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset (E-Gait), which consists of 4,227 human gaits annotated with perceived emotions along with thousands of synthetic gaits. In practice, STEP can learn the affective features and exhibits classification accuracy of 88% on E-Gait, which is 14–30% more accurate over prior methods.
* **Summary**:Our generative model is currently limited to generating gait sequences of a single person. The accuracy of the classification algorithm is also governed by the quality of the video and the pose extraction algorithm. There are many avenues for future work as well. We would like to extend the approach to deal with multi-person or crowd videos. Given the complexity of generating annotated real-world videos, we need better generators to improve the accuracy of classification algorithm. To this end, coupling the generator and the classifier in a semi-supervised manner, similar to that of Bhattacharya et al., is a useful future direction.
* **Keywords**:ST-GCN，Emotion Perception
* **Code**:
* **Dataset**:Emotion-Gait