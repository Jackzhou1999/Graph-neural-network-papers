# Going Deep: Graph Convolutional Ladder-Shape Networks
* **Author**:Ruiqi Hu,Shirui Pan,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang
* **Abstract**:Neighborhood aggregation algorithms like spectral graph convolutional networks (GCNs) formulate graph convolutions as a symmetric Laplacian smoothing operation to aggregate the feature information of one node with that of its neighbors. While they have achieved great success in semisupervised node classification on graphs, current approaches suffer from the over-smoothing problem when the depth of the neural networks increases, which always leads to a noticeable degradation of performance. To solve this problem, we present graph convolutional ladder-shape networks (GCLN), a novel graph neural network architecture that transmits messages from shallow layers to deeper layers to overcome the over-smoothing problem and dramatically extend the scale of the neural networks with improved performance. We have validated the effectiveness of proposed GCLN at a node-wise level with a semi-supervised task (node classification) and an unsupervised task (node clustering), and at a graph-wise level with graph classification by applying a differentiable pooling operation. The proposed GCLN outperforms original GCNs, deep GCNs and other state-of-the-art GCN-based models for all three tasks, which were designed from various perspectives on six real-world benchmark data sets.
* **Summary**:Neighborhood aggregation algorithms inevitably suffer from the over-smoothing problem when the depth of the network is increased, because repeatedly conducting the Laplacian smoothing operation leads to the features of neighboring connected nodes in the graph converging to the same values. In this paper, we have proposed graph convolutional ladder-shape networks (GCLNs) which address the oversmoothing problem with a symmetric ladder-shape architecture. The network consists of a contracting path, expanding path and contextual feature channels which characterize and enhance indistinguishable features by fusing the corresponding contextual information from the contracting side to the deeper layers. A comparison of the results of our experiments on classical and state-of-the-art peers, 8-layer GCN, 8-layer GAT and their residual versions prove the superiority of our method. We have experimentally evaluated the performance of GCLN from the perspective of a node-wise semi-supervised task and node-wise unsupervised task as well as a graphwise task. All the experiments results validate the optimal effectiveness of GCLN from multiple perspectives.
* **Keywords**:Graph Convolutional Ladder-Shape Networks
* **Code**:
* **Dataset**:Cora,Citeseer,Pubmed