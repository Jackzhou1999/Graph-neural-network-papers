# When Does Self-Supervision Help Graph Convolutional Networks?
* **Author**: Yuning You , Tianlong Chen , Zhangyang Wang , Yang Shen
* **Abstract**:Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate selfsupervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel selfsupervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness.
* **Summary**:In this paper, we present a systematic study on the standard and adversarial performances of incorporating selfsupervision into graph convolutional networks (GCNs). We first elaborate three mechanisms by which self-supervision is incorporated into GCNs and rationalize their impacts on the standard performance from the perspective of optimization. Then we focus on multi-task learning and design three novel self-supervised learning tasks. And we rationalize their benefits in generalizable standard performances on various datasets from the perspective or data-driven regularization. Lastly, we integrate multi-task self-supervision into graph adversarial training and show their improving robustness of GCNs against adversarial attacks. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining both generalizability and robustness. Our results also provide rational perspectives toward designing such task forms and incorporation tasks given data characteristics, target tasks and neural network architectures.
* **Keywords**: self-supervision, GCN
* **Code**:https://github.com/Shen-Lab/SS-GCNs
* **Dataset**:Cora, Citeseer, Pubmed