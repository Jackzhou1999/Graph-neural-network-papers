# Generalization and Representational Limits of Graph Neural Networks
* **Author**: Vikas K.Garg, Stefanie Jegelka , Tommi Jaakkola
* **Abstract**:We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties, e.g., shortest/longest cycle, diameter, or certain motifs, cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.
* **Summary**:We introduced H-DCPN primarily to illustrate some ideas that could be used to improve the existing models. In particular, we suggested incorporating geometric information to circumvent some issues with DimeNet. While reasonable for settings where graphs represent actual spatial structures, such models are clearly inapplicable for graphs without any underlying geometric interpretation. Incorporating higher order information via a hypergraph or factor graph might be more natural in some scenarios. Beyond the nature of the graph, a good tradeoff can often be guided by applicationspecific considerations such as size of the training data, computation budget, and constraints (e.g., on training and inference time). Often, additional expressivity from a complex model might be offset by factors such as computational intractability and lack of generalization.
* **Keywords**: Generalization and Representational Limits
* **Code**:
* **Dataset**: