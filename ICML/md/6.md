# GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation
* **Author**: Marc Brockschmidt
* **Abstract**:This paper presents a new Graph Neural Network (GNN) type using feature-wise linear modulation (FiLM). Many standard GNN variants propagate information along the edges of a graph by computing messages based only on the representation of the source of each edge. In GNN-FiLM, the representation of the target node of an edge is used to compute a transformation that can be applied to all incoming messages, allowing featurewise modulation of the passed information. Different GNN architectures are compared in extensive experiments on three tasks from the literature, using re-implementations of many baseline methods. Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between state of the art models are much smaller than reported in the literature and well-known simple baselines that are often not compared to perform better than recently proposed GNN variants. Nonetheless, GNN-FiLM outperforms these methods on a regression task on molecular graphs and performs competitively on other tasks.
* **Summary**:After a review of existing graph neural network architectures, the idea of using hypernetwork-inspired models in the graph setting was explored. This led to two models: Graph Dynamic Convolutional Networks and GNNs with feature-wise linear modulation. While RGDCNs seem to be impractical to train, experiments show that GNN-FiLM outperforms the established baseline models from the literature. However, extensive experiments have shown that the same holds for the simple GNN-MLP definition, which is usally not considered in GNN evaluations.
* **Keywords**: GNN-FiLM
* **Code**:https://github.com/microsoft/tf-gnn-samples
* **Dataset**:Cora/Citeseer/Pubmed, PPI, QM9, VarMisuse