# Simple and Deep Graph Convolutional Networks
* **Author**: Ming Chen , Zhewei Wei , Zengfeng Huang , Bolin Ding , Yaliang Li
* **Abstract**:Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks.
* **Summary**:We propose GCNII, a simple and deep GCN model that prevents over-smoothing by initial residual connection and identity mapping. The theoretical analysis shows that GCNII is able to express a K order polynomial filter with arbitrary coefficients. For vanilla GCN with multiple layers, we provide theoretical and empirical evidence that nodes with higher degrees are more likely to suffer from oversmoothing. Experiments show that the deep GCNII model achieves new state-of-the-art results on various semi- and full-supervised tasks. Interesting directions for future work include combining GCNII with the attention mechanism and analyzing the behavior of GCNII with the ReLU operation.
* **Keywords**: GCNII
* **Code**:https://github.com/chennnM/GCNII
* **Dataset**:Cora, Citeseer, Pubmed, Chameleon, Cornell, Texas, Wisconsin